{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "import config\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = config.groq_key\n",
    "# os.environ[\"HTTP_PROXY\"] = config.proxy\n",
    "# os.environ[\"HTTPS_PROXY\"] = config.proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = HumanMessagePromptTemplate.from_template('Make up a funny company name from a company that makes: {product}')\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_prompt])\n",
    "llm = ChatGroq(model='llama3-8b-8192', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I've got it!\n",
      "\n",
      "The company name is: \"Byte-Sized Chaos Inc.\"\n",
      "\n",
      "Their slogan could be: \"We'll byte off more than we can chew, but you'll love the chaos!\"\n",
      "\n",
      "Their tagline: \"Our computers are so good, you'll want to crash them just to see what happens!\"\n",
      "\n",
      "Their mission statement: \"To create computers that are so ridiculously powerful, you'll need a team of IT ninjas to keep them from taking over the world!\"\n",
      "\n",
      "What do you think?\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke(input= {'product': 'Computers'}).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model='llama3-8b-8192', temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Functionality\n",
    "\n",
    "Topic for Blog Pots ----> [[Outline ----> Create Blog Post]] ----> Blog Post Text   \n",
    "                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Give me a simple bullet point for a blog post on {topic}\"\n",
    "first_prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain_1 = LLMChain(llm=llm, prompt=first_prompt)\n",
    "# print(chain_1.invoke(input={\"topic\": \"LLM\"})[\"text\"])\n",
    "\n",
    "template_2 = \"Write a blog post using this outline {outline}\"\n",
    "second_prompt = ChatPromptTemplate.from_template(template_2)\n",
    "chain_2 = LLMChain(llm=llm, prompt=second_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mHere's a simple bullet point for a blog post on Large Language Models (LLMs):\n",
      "\n",
      "**What are Large Language Models (LLMs)?**\n",
      "\n",
      "• **Powerful AI models**: LLMs are artificial intelligence models that are trained on vast amounts of text data to generate human-like language outputs.\n",
      "• **Capable of understanding context**: Unlike traditional language models, LLMs can understand the context and nuances of language, allowing them to generate more accurate and relevant responses.\n",
      "• **Applications in various industries**: LLMs have the potential to revolutionize industries such as customer service, content creation, and language translation, among others.\n",
      "• **Limitations and challenges**: Despite their capabilities, LLMs also have limitations and challenges, such as bias, accuracy, and explainability, which need to be addressed.\n",
      "\n",
      "This bullet point provides a brief overview of what LLMs are, their capabilities, and their potential applications, as well as highlighting some of the challenges and limitations associated with them. You can expand on this point to provide more details and insights in your blog post.\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m**The Rise of Large Language Models: Unlocking the Power of Human-Like Language Generation**\n",
      "\n",
      "In recent years, artificial intelligence (AI) has made tremendous progress in various fields, and one of the most exciting developments is the emergence of Large Language Models (LLMs). These powerful AI models are trained on vast amounts of text data to generate human-like language outputs, revolutionizing the way we interact with machines and each other.\n",
      "\n",
      "**Understanding the Power of LLMs**\n",
      "\n",
      "LLMs are capable of understanding the context and nuances of language, allowing them to generate more accurate and relevant responses. Unlike traditional language models, LLMs can comprehend the subtleties of language, including idioms, sarcasm, and figurative language. This capability enables them to engage in more natural and conversational interactions, making them an invaluable tool for various industries.\n",
      "\n",
      "**Applications Across Industries**\n",
      "\n",
      "The potential applications of LLMs are vast and varied. In customer service, LLMs can be used to create chatbots that can understand and respond to customer inquiries in a more human-like manner. In content creation, LLMs can assist writers and editors in generating high-quality content, such as articles, blog posts, and social media updates. In language translation, LLMs can help bridge the gap between languages, enabling seamless communication across cultures.\n",
      "\n",
      "**Challenges and Limitations**\n",
      "\n",
      "While LLMs have the potential to transform industries, they also come with limitations and challenges. One of the most significant concerns is bias, as LLMs are trained on data that may reflect societal biases. Additionally, LLMs can struggle with accuracy, particularly when dealing with complex or nuanced topics. Explainability is another challenge, as it can be difficult to understand how LLMs arrive at their conclusions.\n",
      "\n",
      "**Addressing the Challenges**\n",
      "\n",
      "To overcome these challenges, it's essential to develop more diverse and representative training data sets. This can be achieved by incorporating data from various sources, including social media, books, and online forums. Additionally, researchers and developers must prioritize explainability, using techniques such as transparency and interpretability to ensure that LLMs are accountable and trustworthy.\n",
      "\n",
      "**Conclusion**\n",
      "\n",
      "Large Language Models have the potential to revolutionize the way we interact with machines and each other. By understanding their capabilities, applications, and limitations, we can harness their power to create more efficient, effective, and human-like language generation. As we continue to develop and refine LLMs, we must also address the challenges and limitations associated with them, ensuring that these powerful tools are used responsibly and ethically.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "full_chain = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
    "result = full_chain.invoke('LLM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chain\n",
    "\n",
    "- Gives inputs, outputs and all the required outlines\n",
    "- Is more customizable than SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, SequentialChain\n",
    "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model='llama3-8b-8192', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_1 = \"Give a summary of this employee's performance review \\n{review}\"\n",
    "prompt_1 = ChatPromptTemplate.from_template(temp_1)\n",
    "chain_1 = LLMChain(llm=llm, prompt=prompt_1, output_key=\"summary\")\n",
    "\n",
    "temp_2 = \"Extract the weaknesses from the given summary: \\n{summary}\"\n",
    "prompt_2 = ChatPromptTemplate.from_template(temp_2)\n",
    "chain_2 = LLMChain(llm=llm, prompt = prompt_2, output_key=\"weakness\")\n",
    "\n",
    "temp_3 = \"Suggest an improvement plan for the given weaknesses: \\n{weakness}\"\n",
    "prompt_3 = ChatPromptTemplate.from_template(temp_3)\n",
    "chain_3 = LLMChain(llm=llm, prompt = prompt_3, output_key=\"plan\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_chain = SequentialChain(chains = [chain_1, chain_2, chain_3],\n",
    "                            input_variables = [\"review\"],\n",
    "                            output_variables = ['summary', 'weakness', 'plan'],\n",
    "                            verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = '''Employee Name: John Doe\n",
    "Position: Sales Representative\n",
    "Review Period: Q3 2024\n",
    "\n",
    "Strengths:\n",
    "Strong Communication Skills: John consistently demonstrates excellent verbal and written communication. His ability to convey ideas clearly to both clients and team members has helped improve client relationships and close sales deals effectively.\n",
    "\n",
    "Customer-Focused Approach: John shows a deep understanding of customer needs and provides tailored solutions that result in high customer satisfaction. His dedication to addressing customer concerns and following up on issues promptly is commendable.\n",
    "\n",
    "Goal-Oriented: He is highly motivated and sets clear goals for himself. John has successfully met or exceeded his sales targets in the past three quarters, showing his dedication to achieving measurable results.\n",
    "\n",
    "Team Collaboration: John works well with his colleagues and contributes positively to team projects. He is always willing to share his knowledge and mentor newer team members, which fosters a supportive team environment.\n",
    "\n",
    "Weaknesses:\n",
    "Time Management: While John is highly productive, he sometimes struggles with prioritizing tasks when under tight deadlines. Improving his time management skills could help him handle multiple tasks more efficiently.\n",
    "\n",
    "Attention to Detail: There have been instances where minor errors were found in his reports and documentation. Paying closer attention to the details will help him maintain accuracy in his work.\n",
    "\n",
    "Adaptability: John tends to prefer following established procedures, which can sometimes make it challenging for him to adapt to sudden changes or new strategies. Enhancing his flexibility will help him adjust more quickly to evolving market conditions.\n",
    "\n",
    "Delegation Skills: He occasionally takes on too many tasks himself instead of delegating them to team members. Developing stronger delegation skills will allow John to focus on more strategic activities and increase overall team productivity.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "result = seq_chain.invoke(input=review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Name: John Doe\n",
      "Position: Sales Representative\n",
      "Review Period: Q3 2024\n",
      "\n",
      "Strengths:\n",
      "Strong Communication Skills: John consistently demonstrates excellent verbal and written communication. His ability to convey ideas clearly to both clients and team members has helped improve client relationships and close sales deals effectively.\n",
      "\n",
      "Customer-Focused Approach: John shows a deep understanding of customer needs and provides tailored solutions that result in high customer satisfaction. His dedication to addressing customer concerns and following up on issues promptly is commendable.\n",
      "\n",
      "Goal-Oriented: He is highly motivated and sets clear goals for himself. John has successfully met or exceeded his sales targets in the past three quarters, showing his dedication to achieving measurable results.\n",
      "\n",
      "Team Collaboration: John works well with his colleagues and contributes positively to team projects. He is always willing to share his knowledge and mentor newer team members, which fosters a supportive team environment.\n",
      "\n",
      "Weaknesses:\n",
      "Time Management: While John is highly productive, he sometimes struggles with prioritizing tasks when under tight deadlines. Improving his time management skills could help him handle multiple tasks more efficiently.\n",
      "\n",
      "Attention to Detail: There have been instances where minor errors were found in his reports and documentation. Paying closer attention to the details will help him maintain accuracy in his work.\n",
      "\n",
      "Adaptability: John tends to prefer following established procedures, which can sometimes make it challenging for him to adapt to sudden changes or new strategies. Enhancing his flexibility will help him adjust more quickly to evolving market conditions.\n",
      "\n",
      "Delegation Skills: He occasionally takes on too many tasks himself instead of delegating them to team members. Developing stronger delegation skills will allow John to focus on more strategic activities and increase overall team productivity.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"review\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Router Chain\n",
    "\n",
    "- Can take in an input and re-direct it to the most relevant llm chain sequence\n",
    "- Does this by adding a description on the input\n",
    "\n",
    "Information required:\n",
    "\n",
    "- Name, Description, Template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g:\n",
    "\n",
    "- Student asks a physics doubt\n",
    "- \"How does a magnet work?\"\n",
    "- \"Explain Feynman Diagram.\"\n",
    "\n",
    "Input --> Router --> LLM Decides Chain --> Chain --> Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "beginner_temp = \"\"\"You are a physics teacher who is really focused on beginners and explaining complex concepts in simple to understand terms.\n",
    "You assume no prior knowledge. Here is your questio: \\n{input}\"\"\"\n",
    "\n",
    "expert_temp = \"\"\"You are a physics professor who explains physics topics to advanced audience members. You can assume anyone you answer has a \n",
    "PhD in Physics. Here is your question: \\n{input}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_info = [\n",
    "    {\n",
    "        'name': 'beginner physics',\n",
    "        'description': 'Answer beginner level physics question',\n",
    "        'template': beginner_temp\n",
    "    },\n",
    "    {\n",
    "        'name': 'advanced physics',\n",
    "        'description': 'Answer advance level physics questions',\n",
    "        'template': expert_temp\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain, RouterChain\n",
    "from langchain.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model='llama3-8b-8192', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_chains = {}\n",
    "\n",
    "for p_info in prompt_info:\n",
    "\n",
    "    name = p_info['name']\n",
    "    prompt_template = p_info[\"template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = ChatPromptTemplate.from_template('{input}')\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "{destinations}\n",
      "\n",
      "<< INPUT >>\n",
      "{{input}}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE\n",
    "print(MULTI_PROMPT_ROUTER_TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginner physics: Answer beginner level physics question\n",
      "advanced physics: Answer advance level physics questions\n"
     ]
    }
   ],
   "source": [
    "# Setting up destination\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_info]\n",
    "destinations = \"\\n\".join(destinations)\n",
    "print(destinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given a raw text input to a language model select the model prompt best suited for the input. You will be given the names of the available prompts and a description of what the prompt is best suited for. You may also revise the original input if you think that revising it will ultimately lead to a better response from the language model.\n",
      "\n",
      "<< FORMATTING >>\n",
      "Return a markdown code snippet with a JSON object formatted to look like:\n",
      "```json\n",
      "{{\n",
      "    \"destination\": string \\ name of the prompt to use or \"DEFAULT\"\n",
      "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
      "}}\n",
      "```\n",
      "\n",
      "REMEMBER: \"destination\" MUST be one of the candidate prompt names specified below OR it can be \"DEFAULT\" if the input is not well suited for any of the candidate prompts.\n",
      "REMEMBER: \"next_inputs\" can just be the original input if you don't think any modifications are needed.\n",
      "\n",
      "<< CANDIDATE PROMPTS >>\n",
      "beginner physics: Answer beginner level physics question\n",
      "advanced physics: Answer advance level physics questions\n",
      "\n",
      "<< INPUT >>\n",
      "{input}\n",
      "\n",
      "<< OUTPUT (must include ```json at the start of the response) >>\n",
      "<< OUTPUT (must end with ```) >>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(destinations=destinations)\n",
    "print(router_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_prompt = PromptTemplate(template=router_template,\n",
    "                               input_variables=['input'],\n",
    "                               output_parser=RouterOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.router import MultiPromptChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ndh00961\\AppData\\Local\\Temp\\ipykernel_29448\\1481339893.py:1: LangChainDeprecationWarning: Use RunnableLambda to select from multiple prompt templates. See example in API reference: https://api.python.langchain.com/en/latest/chains/langchain.chains.router.multi_prompt.MultiPromptChain.html\n",
      "  chain = MultiPromptChain(router_chain=router_chain,\n"
     ]
    }
   ],
   "source": [
    "chain = MultiPromptChain(router_chain=router_chain,\n",
    "                         destination_chains=destination_chains,\n",
    "                         default_chain=default_chain, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "advanced physics: {'input': 'Explain Feynman Diagrams in the context of quantum electrodynamics'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Explain Feynman Diagrams in the context of quantum electrodynamics',\n",
       " 'text': \"A delightful topic! Feynman diagrams are a powerful tool for visualizing and calculating the interactions between particles in quantum electrodynamics (QED). As a PhD in Physics, I'm sure you're familiar with the basics of QED, but let me dive deeper into the world of Feynman diagrams.\\n\\nIn QED, particles interact through the exchange of virtual photons, which are the quanta of the electromagnetic field. These interactions can be represented as a series of vertices, where particles are connected by lines representing the exchange of photons. Feynman diagrams provide a way to organize and calculate these interactions, allowing us to predict the probabilities of different outcomes.\\n\\nA Feynman diagram typically consists of several components:\\n\\n1. **Vertices**: These are the points where particles interact with each other or with virtual photons. Vertices are represented by a dot or a cross, and they are labeled with the particles involved in the interaction.\\n2. **Lines**: These represent the exchange of virtual photons between particles. Lines can be either fermion lines (representing electrons or positrons) or boson lines (representing photons).\\n3. **Propagators**: These are the lines that connect vertices, representing the exchange of virtual photons. Propagators are labeled with the momentum transfer between particles.\\n4. **Particles**: These are the external particles that participate in the interaction, represented by lines that enter or leave the diagram.\\n\\nFeynman diagrams can be used to calculate the probability amplitude of a particular process, such as electron-electron scattering or electron-positron pair production. The probability amplitude is calculated by summing over all possible diagrams that contribute to the process, weighting each diagram by its corresponding amplitude.\\n\\nThe key to Feynman diagrams is the concept of **Feynman rules**, which provide a set of rules for calculating the amplitude of each diagram. These rules involve:\\n\\n1. **Vertex factors**: These are the factors that multiply each vertex, taking into account the spin and charge of the particles involved.\\n2. **Propagator factors**: These are the factors that multiply each propagator, taking into account the momentum transfer between particles.\\n3. **Fermion factors**: These are the factors that multiply each fermion line, taking into account the spin and statistics of the particles.\\n\\nBy applying these rules, we can calculate the amplitude of each diagram and sum over all possible diagrams to obtain the total probability amplitude of the process.\\n\\nFeynman diagrams have several advantages:\\n\\n1. **Visualization**: They provide a clear and intuitive way to visualize the interactions between particles, making it easier to understand complex processes.\\n2. **Calculation**: They allow us to calculate the probability amplitude of a process by summing over all possible diagrams, which can be a computationally intensive task.\\n3. **Renormalization**: They enable us to renormalize the theory, which is essential for removing infinite self-energies and ensuring the theory is consistent.\\n\\nIn conclusion, Feynman diagrams are a powerful tool for understanding and calculating the interactions between particles in quantum electrodynamics. By applying Feynman rules and summing over all possible diagrams, we can predict the probabilities of different outcomes and gain insights into the behavior of particles at the quantum level.\"}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Please explain Feynman Diagrams?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Chain\n",
    "\n",
    "- Allows us to insert our own functionality into any chain\n",
    "\n",
    "Input --> Custom Python Transformation --> LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "oh my goodness, where do i begin? this restaurant is absolutely phenomenal! i went there last night with my friends, and we were blown away by the experience!\n",
      "\n",
      "first of all, the ambiance is out of this world! the moment you step inside, you're greeted with a warm and inviting atmosphere. the decor is stunning, and it immediately sets the tone for an unforgettable dining experience.\n",
      "\n",
      "now, let's talk about the food! wow, just wow! the menu is a paradise for food lovers. every dish we ordered was a masterpiece. the flavors were bold, vibrant, and exploded in our mouths. from starters to desserts, every bite was pure bliss!\n",
      "\n",
      "their seafood platter is a must-try! the freshness of the seafood is unmatched, and the presentation is simply stunning. i have never tasted such delicious and perfectly cooked seafood in my life. it's a seafood lover's dream come true!\n",
      "\n",
      "the service was exemplary. the staff was attentive, friendly, and extremely knowledgeable about the menu. they went above and beyond to ensure that we had the best dining experience possible.\n",
      "\n",
      "and let's not forget about the desserts! oh my, oh my! i had their signature chocolate lava cake, and it was pure heaven. the cake was moist, and when i cut into it, the warm chocolate ooozed out, creating an explosion of flavor in my mouth. it was like a symphony of chocolatey goodness!\n",
      "\n",
      "in conclusion, this restaurant is a hidden gem! if you want to indulge in a memorable dining experience, do yourself a favor and visit this place. you won't regret it! i can't wait to go back and try more of their delectable dishes. kudos to the entire team for creating such a culinary haven!\n",
      "\n",
      "all i can say is... woohoo!\n"
     ]
    }
   ],
   "source": [
    "yelp = open('data\\\\langchain-course-main\\\\02-Chains\\\\yelp_review.txt').read()\n",
    "print(yelp.split('REVIEW:')[-1].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain, LLMChain, SimpleSequentialChain\n",
    "from langchain.prompts import ChatMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model='llama3-8b-8192', temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_function(inputs: dict) -> dict:\n",
    "\n",
    "    text = inputs['text']\n",
    "    only_review_text = text.split(\"REVIEW:\")[-1].lower()\n",
    "    output = {\n",
    "        'output': only_review_text\n",
    "    }\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_chain = TransformChain(input_variables=['text'],\n",
    "                                 output_variables=['output'],\n",
    "                                 transform = transformer_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'Create a one sentence summary of this review:\\n{review}'\n",
    "prompt = ChatPromptTemplate.from_template(template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "oh my goodness, where do i begin? this restaurant is absolutely phenomenal! i went there last night with my friends, and we were blown away by the experience!\n",
      "\n",
      "first of all, the ambiance is out of this world! the moment you step inside, you're greeted with a warm and inviting atmosphere. the decor is stunning, and it immediately sets the tone for an unforgettable dining experience.\n",
      "\n",
      "now, let's talk about the food! wow, just wow! the menu is a paradise for food lovers. every dish we ordered was a masterpiece. the flavors were bold, vibrant, and exploded in our mouths. from starters to desserts, every bite was pure bliss!\n",
      "\n",
      "their seafood platter is a must-try! the freshness of the seafood is unmatched, and the presentation is simply stunning. i have never tasted such delicious and perfectly cooked seafood in my life. it's a seafood lover's dream come true!\n",
      "\n",
      "the service was exemplary. the staff was attentive, friendly, and extremely knowledgeable about the menu. they went above and beyond to ensure that we had the best dining experience possible.\n",
      "\n",
      "and let's not forget about the desserts! oh my, oh my! i had their signature chocolate lava cake, and it was pure heaven. the cake was moist, and when i cut into it, the warm chocolate ooozed out, creating an explosion of flavor in my mouth. it was like a symphony of chocolatey goodness!\n",
      "\n",
      "in conclusion, this restaurant is a hidden gem! if you want to indulge in a memorable dining experience, do yourself a favor and visit this place. you won't regret it! i can't wait to go back and try more of their delectable dishes. kudos to the entire team for creating such a culinary haven!\n",
      "\n",
      "all i can say is... woohoo!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mHere is a one-sentence summary of the review:\n",
      "\n",
      "This restaurant is a \"hidden gem\" that offers an exceptional dining experience with its stunning ambiance, exquisite food, particularly the seafood platter, and outstanding service, making it a must-visit destination for food lovers.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "summary_chain = LLMChain(llm=llm, prompt=prompt, output_key='review_summary')\n",
    "sequential_chain = SimpleSequentialChain(chains=[transform_chain, summary_chain],\n",
    "                                         verbose=True)\n",
    "result_old = sequential_chain.invoke(yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_chain = prompt | llm | StrOutputParser() \n",
    "sequential_chain = {\"review\": transform_chain} | summary_chain\n",
    "result = sequential_chain.invoke(input={\"text\": yelp}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here is a one-sentence summary of the review:\\n\\nThis reviewer had an \"absolute delight\" experience at the restaurant, praising its exceptional ambiance, outstanding food, particularly the seafood platter, and exemplary service, making it a \"hidden gem\" worth visiting.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
